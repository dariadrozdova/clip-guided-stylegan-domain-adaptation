{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "yu26-YmRc3J5",
      "metadata": {
        "id": "yu26-YmRc3J5"
      },
      "source": [
        "# CLIP-направляемая доменная адаптация StyleGAN-2\n",
        "\n",
        "\n",
        "В ноутбуке реализуется упрощённый вариант метода StyleGAN-NADA (No-Data Domain Adaptation) для адаптации предобученного генератора StyleGAN-2 под новый визуальный домен (стиль sketch) без использования изображений целевого домена.\n",
        "\n",
        "В качестве исходной модели используется предобученный генератор StyleGAN-2, обученный на датасете FFHQ. Для адаптации создаются две копии генератора: замороженная и обучаемая. Обучение проводится таким образом, чтобы для одного и того же латентного вектора изображения, сгенерированные обучаемой моделью, смещались в сторону целевого домена относительно изображений, полученных из замороженной модели.\n",
        "\n",
        "Направление адаптации задаётся в пространстве CLIP через текстовые описания исходного и целевого доменов. Обучение осуществляется без дискриминатора и без использования изображений целевого домена, что соответствует постановке задачи no-data domain adaptation.\n",
        "\n",
        "Все промежуточные результаты сохраняются для последующего анализа и продолжения экспериментов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "atzTYvH-nIoO",
      "metadata": {
        "id": "atzTYvH-nIoO"
      },
      "outputs": [],
      "source": [
        "import os, json, sys\n",
        "from datetime import datetime\n",
        "import importlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import csv\n",
        "import glob\n",
        "from PIL import Image\n",
        "from contextlib import contextmanager\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ftfy regex tqdm pillow\n",
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(\"stylegan2-ada-pytorch\"):\n",
        "    !git clone -q https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "\n",
        "sys.path.append(\"/content/stylegan2-ada-pytorch\")\n"
      ],
      "metadata": {
        "id": "f0zJbUkcjIbV"
      },
      "id": "f0zJbUkcjIbV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip"
      ],
      "metadata": {
        "id": "zdBJxZwWjIMD"
      },
      "id": "zdBJxZwWjIMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Исследование важности блоков\n",
        "\n",
        "Ранние блоки StyleGAN-2 отвечают за пропороции лица, расположение основных частей. Средние - за более детальную структуру, форму глаз, носа, рта. Поздние блоки в основном контролируют контуры, текстуры, цвет, мелкие детали.\n",
        "\n",
        "Так как стиль \"скетч\" не предполагает изменения пропорций лица, а влияет в первую очередь на линии, контуры и визуальную текстуру, попробуем разморозить только поздние блоки - последние 4 блока."
      ],
      "metadata": {
        "id": "zxCWjFWay84R"
      },
      "id": "zxCWjFWay84R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "th6qx0dFc3J-",
      "metadata": {
        "id": "th6qx0dFc3J-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/stylegan_nada_project\"\n",
        "RUN_NAME = \"sketch_late_only_v0\"\n",
        "RUN_DIR = os.path.join(BASE_DIR, RUN_NAME)\n",
        "CKPT_DIR = os.path.join(RUN_DIR, \"checkpoints\")\n",
        "SAMPLES_DIR = os.path.join(RUN_DIR, \"samples\")\n",
        "LOGS_DIR = os.path.join(RUN_DIR, \"logs\")\n",
        "\n",
        "for d in [RUN_DIR, CKPT_DIR, SAMPLES_DIR, LOGS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "config = {\n",
        "    \"run_name\": RUN_NAME,\n",
        "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "    \"source_prompt\": \"photo of a face\",\n",
        "    \"target_prompt\": \"sketch portrait of a face\",\n",
        "    \"size\": 1024,\n",
        "    \"truncation\": 0.7,\n",
        "    \"batch_size\": 1,\n",
        "    \"max_steps\": 500,\n",
        "    \"save_every\": 50,\n",
        "    \"lr\": 2e-4,\n",
        "    \"adam_betas\": [0.0, 0.99],\n",
        "    \"late_k_blocks\": 4,\n",
        "    \"fixed_seeds\": [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "}\n",
        "\n",
        "CONFIG_PATH = os.path.join(RUN_DIR, \"config.json\")\n",
        "with open(CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nbzaeM3HjNe_",
      "metadata": {
        "id": "nbzaeM3HjNe_"
      },
      "outputs": [],
      "source": [
        "PKL_PATH = \"/content/ffhq.pkl\"\n",
        "if not os.path.exists(PKL_PATH):\n",
        "    !wget -q -O /content/ffhq.pkl https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rD2cIGPUnj3V",
      "metadata": {
        "id": "rD2cIGPUnj3V"
      },
      "outputs": [],
      "source": [
        "REPO_DIR = \"/content/stylegan2-ada-pytorch\"\n",
        "\n",
        "assert os.path.isdir(REPO_DIR)\n",
        "assert os.path.isdir(os.path.join(REPO_DIR, \"dnnlib\"))\n",
        "assert os.path.isfile(os.path.join(REPO_DIR, \"legacy.py\"))\n",
        "\n",
        "sys.path = [p for p in sys.path if p != REPO_DIR]\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "for m in list(sys.modules.keys()):\n",
        "    if m == \"dnnlib\" or m.startswith(\"dnnlib.\") or m == \"legacy\":\n",
        "        del sys.modules[m]\n",
        "\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "import dnnlib\n",
        "import legacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lqWO64tOc3KA",
      "metadata": {
        "id": "lqWO64tOc3KA"
      },
      "source": [
        "Инициализация генератора и генерация baseline-изображений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LNsfp0aTN-dg",
      "metadata": {
        "id": "LNsfp0aTN-dg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Failed to build CUDA kernels for upfirdn2d.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Failed to build CUDA kernels for bias_act.*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OAVObGc7oCWR",
      "metadata": {
        "id": "OAVObGc7oCWR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(PKL_PATH, \"rb\") as f:\n",
        "    net = legacy.load_network_pkl(f)\n",
        "\n",
        "G_ema = net[\"G_ema\"].to(device).eval()\n",
        "\n",
        "def seed_to_z(seed: int, z_dim: int, device: torch.device) -> torch.Tensor:\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    z = rnd.randn(1, z_dim).astype(np.float32)\n",
        "    return torch.from_numpy(z).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_img(G, z, truncation_psi: float):\n",
        "    c = None\n",
        "    img = G(z, c, truncation_psi=truncation_psi, noise_mode=\"const\")\n",
        "    img = (img * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    img = img[0].permute(1, 2, 0).cpu().numpy()\n",
        "    return img\n",
        "\n",
        "def make_grid(images, cols=8):\n",
        "    rows = int(np.ceil(len(images) / cols))\n",
        "    h, w, _ = images[0].shape\n",
        "    grid = np.zeros((rows * h, cols * w, 3), dtype=np.uint8)\n",
        "    for idx, im in enumerate(images):\n",
        "        r = idx // cols\n",
        "        c = idx % cols\n",
        "        grid[r*h:(r+1)*h, c*w:(c+1)*w] = im\n",
        "    return grid\n",
        "\n",
        "baseline_imgs = []\n",
        "for s in config[\"fixed_seeds\"]:\n",
        "    z = seed_to_z(s, G_ema.z_dim, device)\n",
        "    baseline_imgs.append(gen_img(G_ema, z, truncation_psi=config[\"truncation\"]))\n",
        "\n",
        "baseline_grid = make_grid(baseline_imgs, cols=8)\n",
        "baseline_path = os.path.join(SAMPLES_DIR, \"baseline_frozen.png\")\n",
        "Image.fromarray(baseline_grid).save(baseline_path)\n",
        "\n",
        "from IPython.display import display\n",
        "display(Image.open(baseline_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9k_CFsqwc3KB",
      "metadata": {
        "id": "9k_CFsqwc3KB"
      },
      "source": [
        "## Загрузка CLIP и подготовка эмбеддингов текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eguftsSpc3KC",
      "metadata": {
        "id": "eguftsSpc3KC"
      },
      "outputs": [],
      "source": [
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "clip_model.eval()\n",
        "\n",
        "CLIP_MEAN = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device).view(1,3,1,1)\n",
        "CLIP_STD  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device).view(1,3,1,1)\n",
        "\n",
        "def to_clip_input(img_nchw_m1p1: torch.Tensor) -> torch.Tensor:\n",
        "    x = (img_nchw_m1p1 + 1.0) / 2.0\n",
        "    x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
        "    x = (x - CLIP_MEAN) / CLIP_STD\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_text_embed(text: str) -> torch.Tensor:\n",
        "    tokens = clip.tokenize([text]).to(device)\n",
        "    emb = clip_model.encode_text(tokens).float()\n",
        "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    return emb[0]\n",
        "\n",
        "def clip_image_embed(img_nchw_m1p1: torch.Tensor) -> torch.Tensor:\n",
        "    x = to_clip_input(img_nchw_m1p1)\n",
        "    emb = clip_model.encode_image(x).float()\n",
        "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    return emb\n",
        "\n",
        "t_src = clip_text_embed(config[\"source_prompt\"])\n",
        "t_tgt = clip_text_embed(config[\"target_prompt\"])\n",
        "\n",
        "d_txt = (t_tgt - t_src)\n",
        "d_txt = d_txt / (d_txt.norm() + 1e-8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cDQejkP0c3KC",
      "metadata": {
        "id": "cDQejkP0c3KC"
      },
      "source": [
        "## Подготовка обучаемой и замороженной копий генератора, разморозка последних блоков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NCRTesACc3KC",
      "metadata": {
        "id": "NCRTesACc3KC"
      },
      "outputs": [],
      "source": [
        "G_frozen = copy.deepcopy(G_ema).to(device).eval()\n",
        "G_train  = copy.deepcopy(G_ema).to(device).train()\n",
        "\n",
        "for p in G_train.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "k = int(config[\"late_k_blocks\"])\n",
        "\n",
        "resolutions = list(G_train.synthesis.block_resolutions)\n",
        "late_resolutions = resolutions[-k:]\n",
        "\n",
        "for r in late_resolutions:\n",
        "    block = getattr(G_train.synthesis, f\"b{r}\")\n",
        "    for p in block.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "for p in G_train.mapping.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "trainable = sum(p.numel() for p in G_train.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in G_train.parameters())\n",
        "trainable, total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l7rvGMT_c3KC",
      "metadata": {
        "id": "l7rvGMT_c3KC"
      },
      "source": [
        "## Оптимизатор, функция потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KBZPXKE5c3KD",
      "metadata": {
        "id": "KBZPXKE5c3KD"
      },
      "outputs": [],
      "source": [
        "train_params = [p for p in G_train.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(train_params, lr=config[\"lr\"], betas=tuple(config[\"adam_betas\"]))\n",
        "\n",
        "def directional_clip_loss(img_train_m1p1: torch.Tensor, img_frozen_m1p1: torch.Tensor, d_txt: torch.Tensor) -> torch.Tensor:\n",
        "    e_train = clip_image_embed(img_train_m1p1)\n",
        "    e_froz  = clip_image_embed(img_frozen_m1p1)\n",
        "\n",
        "    d_img = (e_train - e_froz)\n",
        "    d_img = d_img / (d_img.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "    d_txt_b = d_txt.view(1, -1).expand_as(d_img)\n",
        "    cos = (d_img * d_txt_b).sum(dim=-1)\n",
        "    return (1.0 - cos).mean()\n",
        "\n",
        "fixed_z = torch.cat([seed_to_z(s, 512, device) for s in config[\"fixed_seeds\"]], dim=0)\n",
        "\n",
        "LOSS_CSV = os.path.join(LOGS_DIR, \"loss.csv\")\n",
        "if not os.path.exists(LOSS_CSV):\n",
        "    with open(LOSS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"step\", \"loss\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hxHZNr3Yc3KD",
      "metadata": {
        "id": "hxHZNr3Yc3KD"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MjKb25xIOrm3",
      "metadata": {
        "id": "MjKb25xIOrm3"
      },
      "outputs": [],
      "source": [
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
        "\n",
        "LOSS_CSV = os.path.join(RUN_DIR, \"loss.csv\")\n",
        "if not os.path.exists(LOSS_CSV):\n",
        "    with open(LOSS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"step\", \"loss\"])\n",
        "\n",
        "max_steps = int(config[\"max_steps\"])\n",
        "save_every = int(config[\"save_every\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6qcYjX7IOurk",
      "metadata": {
        "id": "6qcYjX7IOurk"
      },
      "outputs": [],
      "source": [
        "def latest_checkpoint_path():\n",
        "    ckpts = sorted(glob.glob(os.path.join(CKPT_DIR, \"checkpoint_*.pt\")))\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "def save_checkpoint(step: int):\n",
        "    path = os.path.join(CKPT_DIR, f\"checkpoint_{step:06d}.pt\")\n",
        "    torch.save({\n",
        "        \"step\": step,\n",
        "        \"G_train\": G_train.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"config\": config,\n",
        "        \"fixed_z\": fixed_z.detach().cpu(),\n",
        "    }, path)\n",
        "    return path\n",
        "\n",
        "def load_latest_checkpoint():\n",
        "    ckpt_path = latest_checkpoint_path()\n",
        "    if not ckpt_path:\n",
        "        return 0\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    G_train.load_state_dict(ckpt[\"G_train\"], strict=False)\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    return int(ckpt[\"step\"]) + 1\n",
        "\n",
        "start_step = load_latest_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fw9SXeLCQM3I",
      "metadata": {
        "id": "Fw9SXeLCQM3I"
      },
      "outputs": [],
      "source": [
        "def to_uint8_hwc(img_nchw_m1p1: torch.Tensor) -> np.ndarray:\n",
        "    x = (img_nchw_m1p1.clamp(-1, 1) + 1.0) / 2.0\n",
        "    x = (x * 255.0).round().to(torch.uint8)\n",
        "    x = x[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xngq65ygO0nB",
      "metadata": {
        "id": "Xngq65ygO0nB"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def make_sample_grid(step: int):\n",
        "    imgs_f = []\n",
        "    imgs_t = []\n",
        "\n",
        "    for i in range(fixed_z.shape[0]):\n",
        "        z = fixed_z[i:i+1]\n",
        "\n",
        "        img_f = G_frozen(z, None, truncation_psi=config[\"truncation\"], noise_mode=\"const\")\n",
        "        img_t = G_train.eval()(z, None, truncation_psi=config[\"truncation\"], noise_mode=\"const\")\n",
        "\n",
        "        imgs_f.append(to_uint8_hwc(img_f))\n",
        "        imgs_t.append(to_uint8_hwc(img_t))\n",
        "\n",
        "    G_train.train()\n",
        "\n",
        "    grid_f = make_grid(imgs_f, cols=8)\n",
        "    grid_t = make_grid(imgs_t, cols=8)\n",
        "    grid = np.concatenate([grid_f, grid_t], axis=0)\n",
        "\n",
        "    out_path = os.path.join(SAMPLES_DIR, f\"sample_{step:06d}.png\")\n",
        "    Image.fromarray(grid).save(out_path)\n",
        "    return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hXir-BzlQXxI",
      "metadata": {
        "id": "hXir-BzlQXxI"
      },
      "outputs": [],
      "source": [
        "def train_one_step():\n",
        "    z = torch.randn([config[\"batch_size\"], G_train.z_dim], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_frozen = G_frozen(z, None, truncation_psi=config[\"truncation\"], noise_mode=\"const\")\n",
        "\n",
        "    img_train = G_train(z, None, truncation_psi=config[\"truncation\"], noise_mode=\"const\")\n",
        "\n",
        "    loss = directional_clip_loss(img_train, img_frozen, d_txt)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return float(loss.item())\n",
        "\n",
        "train_one_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pFRySnPlQ-gJ",
      "metadata": {
        "id": "pFRySnPlQ-gJ"
      },
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def suppress_output():\n",
        "    devnull = open(os.devnull, \"w\")\n",
        "    old_stdout, old_stderr = sys.stdout, sys.stderr\n",
        "    try:\n",
        "        sys.stdout, sys.stderr = devnull, devnull\n",
        "        yield\n",
        "    finally:\n",
        "        sys.stdout, sys.stderr = old_stdout, old_stderr\n",
        "        devnull.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpIwfeTpl6Cv",
      "metadata": {
        "id": "rpIwfeTpl6Cv"
      },
      "outputs": [],
      "source": [
        "!pip -q install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJ8AbfLipId4",
      "metadata": {
        "id": "yJ8AbfLipId4"
      },
      "outputs": [],
      "source": [
        "start_step = 0\n",
        "\n",
        "for p in glob.glob(os.path.join(CKPT_DIR, \"checkpoint_*.pt\")):\n",
        "    os.remove(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdlniYOco1D4",
      "metadata": {
        "id": "fdlniYOco1D4"
      },
      "outputs": [],
      "source": [
        "print(\"start_step =\", start_step)\n",
        "print(\"max_steps  =\", max_steps)\n",
        "print(\"range length =\", len(list(range(start_step, max_steps + 1))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J26-V9qVRLk-",
      "metadata": {
        "id": "J26-V9qVRLk-"
      },
      "outputs": [],
      "source": [
        "log_every = int(config.get(\"log_every\", 10))\n",
        "preview_every = int(config.get(\"preview_every\", save_every))\n",
        "\n",
        "pbar = tqdm(range(start_step, max_steps + 1), desc=\"Обучение\", leave=True)\n",
        "\n",
        "for step in pbar:\n",
        "    with suppress_output():\n",
        "        loss_value = train_one_step()\n",
        "\n",
        "    with open(LOSS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([step, loss_value])\n",
        "\n",
        "    if step % log_every == 0 or step == start_step:\n",
        "        pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\"})\n",
        "\n",
        "    if step % preview_every == 0 or step == max_steps:\n",
        "        with suppress_output():\n",
        "            out_path = make_sample_grid(step)\n",
        "            save_checkpoint(step)\n",
        "\n",
        "        img = Image.open(out_path)\n",
        "        img.thumbnail((700, 700))\n",
        "        display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Редактирование сгенерированных изображений\n",
        "\n",
        "Для демонстрации редактирования сгенерированных изображений используются фиксированные латентные векторы z. Для каждого такого вектора сравниваются изображения, полученные из замороженной копии генератора (G_frozen) и обучаемой копии генератора (G_train). Это позволяет наглядно показать, как меняется визуальный стиль изображения при сохранении его структуры и идентичности. Таким образом, реализуется редактирование уже сгенерированных изображений, а не независимая генерация новых примеров."
      ],
      "metadata": {
        "id": "465tC2Na1hog"
      },
      "id": "465tC2Na1hog"
    },
    {
      "cell_type": "code",
      "source": [
        "def latest_checkpoint_path():\n",
        "    ckpts = sorted(glob.glob(os.path.join(CKPT_DIR, \"checkpoint_*.pt\")))\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "with open(PKL_PATH, \"rb\") as f:\n",
        "    net = legacy.load_network_pkl(f)\n",
        "\n",
        "G_ema = net[\"G_ema\"].to(device).eval()\n",
        "\n",
        "G_frozen = copy.deepcopy(G_ema).to(device).eval()\n",
        "G_train  = copy.deepcopy(G_ema).to(device).eval()\n",
        "\n",
        "ckpt_path = latest_checkpoint_path()\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "G_train.load_state_dict(ckpt[\"G_train\"], strict=False)\n",
        "\n",
        "fixed_z = ckpt[\"fixed_z\"].to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_pairs_grid(G_frozen, G_train, fixed_z, truncation_psi, out_path, cols=8, thumb=(900, 900)):\n",
        "    imgs = []\n",
        "    for i in range(fixed_z.shape[0]):\n",
        "        z = fixed_z[i:i+1]\n",
        "\n",
        "        img_f = G_frozen(z, None, truncation_psi=truncation_psi, noise_mode=\"const\")\n",
        "        img_t = G_train(z, None, truncation_psi=truncation_psi, noise_mode=\"const\")\n",
        "\n",
        "        imgs.append(to_uint8_hwc(img_f))\n",
        "        imgs.append(to_uint8_hwc(img_t))\n",
        "\n",
        "    grid = make_grid(imgs, cols=2)\n",
        "    Image.fromarray(grid).save(out_path)\n",
        "\n",
        "    im = Image.open(out_path)\n",
        "    im.thumbnail(thumb)\n",
        "    return im\n",
        "\n",
        "out_path = os.path.join(SAMPLES_DIR, \"edited_generated_images_from_checkpoint.png\")\n",
        "preview = make_pairs_grid(\n",
        "    G_frozen=G_frozen,\n",
        "    G_train=G_train,\n",
        "    fixed_z=fixed_z,\n",
        "    truncation_psi=config[\"truncation\"],\n",
        "    out_path=out_path,\n",
        "    cols=8\n",
        ")\n",
        "\n",
        "display(preview)"
      ],
      "metadata": {
        "id": "Y4QQB6Tu10TB"
      },
      "id": "Y4QQB6Tu10TB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "TB7iPW_3c3KD",
      "metadata": {
        "id": "TB7iPW_3c3KD"
      },
      "source": [
        "### Анализ генераций, полученных на разных этапах обучения"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sorted(glob.glob(os.path.join(SAMPLES_DIR, \"sample_*.png\")))\n",
        "\n",
        "idx_middle = len(samples) // 2\n",
        "idx_last = len(samples) - 1\n",
        "\n",
        "selected = [\n",
        "    (\"Середина обучения\", samples[idx_middle]),\n",
        "    (\"Конец обучения\", samples[idx_last]),\n",
        "]\n",
        "\n",
        "for title, path in selected:\n",
        "    img = Image.open(path)\n",
        "    print(title)\n",
        "    display(img)\n"
      ],
      "metadata": {
        "id": "3adFiQs9nUdc"
      },
      "id": "3adFiQs9nUdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "AosS0DrIwc-r",
      "metadata": {
        "id": "AosS0DrIwc-r"
      },
      "source": [
        "**Вывод**\n",
        "\n",
        "В данном эксперименте визуальное качество результатов улучшалось на протяжении всего процесса обучения, и наиболее выраженный скетч-эффект наблюдается на финальных шагах оптимизации. Разморозка 4-х последний блоков дала неплохой, но всё ещё слабый результат.\n",
        "\n",
        "В качестве следующего шага планируется разморозить и обучить дополнительные слои, что позволит оценить их влияние на выраженность скетч-стиля."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение дополнительных слоёв"
      ],
      "metadata": {
        "id": "hD72EKNf30QO"
      },
      "id": "hD72EKNf30QO"
    },
    {
      "cell_type": "code",
      "source": [
        "# новые директории для эксперимента\n",
        "EXP_TAG = \"exp2_more_layers\"\n",
        "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "RUN_DIR_2 = os.path.join(os.path.dirname(RUN_DIR), f\"{os.path.basename(RUN_DIR)}_{EXP_TAG}_{STAMP}\")\n",
        "CKPT_DIR_2 = os.path.join(RUN_DIR_2, \"checkpoints\")\n",
        "SAMPLES_DIR_2 = os.path.join(RUN_DIR_2, \"samples\")\n",
        "\n",
        "os.makedirs(CKPT_DIR_2, exist_ok=True)\n",
        "os.makedirs(SAMPLES_DIR_2, exist_ok=True)\n",
        "\n",
        "LOSS_CSV_2 = os.path.join(RUN_DIR_2, \"loss.csv\")\n",
        "CONFIG_PATH_2 = os.path.join(RUN_DIR_2, \"config.json\")\n",
        "\n",
        "with open(CONFIG_PATH_2, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "1MYQ-p_O35r1"
      },
      "id": "1MYQ-p_O35r1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_DIR = CKPT_DIR_2\n",
        "SAMPLES_DIR = SAMPLES_DIR_2\n",
        "LOSS_CSV = LOSS_CSV_2\n",
        "CONFIG_PATH = CONFIG_PATH_2"
      ],
      "metadata": {
        "id": "JDw0czD14CJD"
      },
      "id": "JDw0czD14CJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В прошлом эксперименте мы обучали 4 последних слоя, на этот раз попробуем обучить 6 слоёв."
      ],
      "metadata": {
        "id": "JXPWhSwy4xXq"
      },
      "id": "JXPWhSwy4xXq"
    },
    {
      "cell_type": "code",
      "source": [
        "k_new = 6\n",
        "config[\"late_k_blocks\"] = k_new\n",
        "\n",
        "for p in G_train.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "resolutions = list(G_train.synthesis.block_resolutions)\n",
        "late_resolutions = resolutions[-k_new:]\n",
        "\n",
        "for r in late_resolutions:\n",
        "    block = getattr(G_train.synthesis, f\"b{r}\")\n",
        "    for p in block.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "for p in G_train.mapping.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    [p for p in G_train.parameters() if p.requires_grad],\n",
        "    lr=float(config[\"lr\"]),\n",
        "    betas=tuple(config[\"adam_betas\"])\n",
        ")"
      ],
      "metadata": {
        "id": "fqF6Z679431E"
      },
      "id": "fqF6Z679431E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = int(config.get(\"log_every\", 10))\n",
        "preview_every = int(config.get(\"preview_every\", save_every))\n",
        "\n",
        "pbar = tqdm(range(start_step, max_steps + 1), desc=\"Обучение k=6\", leave=True)\n",
        "\n",
        "for step in pbar:\n",
        "    with suppress_output():\n",
        "        loss_value = train_one_step()\n",
        "\n",
        "    with open(LOSS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([step, float(loss_value)])\n",
        "\n",
        "    if step % log_every == 0 or step == start_step:\n",
        "        pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\"})\n",
        "\n",
        "    if step % preview_every == 0 or step == max_steps:\n",
        "        with suppress_output():\n",
        "            out_path = make_sample_grid(step)\n",
        "            save_checkpoint(step)\n",
        "\n",
        "        img = Image.open(out_path)\n",
        "        img.thumbnail((700, 700))\n",
        "        display(img)"
      ],
      "metadata": {
        "id": "84rqUJlh5xg_"
      },
      "id": "84rqUJlh5xg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сравнение исходных изображений и результата обучения 6 последних блоков"
      ],
      "metadata": {
        "id": "2i-Z0X6T8n7n"
      },
      "id": "2i-Z0X6T8n7n"
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLES_DIR_K6 = \"/content/drive/MyDrive/stylegan_nada_project/sketch_late_only_v0_exp2_more_layers_20260202_010304/samples\"\n",
        "\n",
        "samples_k6 = sorted(glob.glob(os.path.join(SAMPLES_DIR_K6, \"sample_*.png\")))\n",
        "final_k6_path = samples_k6[-1]\n",
        "\n",
        "display(Image.open(final_k6_path))\n"
      ],
      "metadata": {
        "id": "3-mcz8vy9p9s"
      },
      "id": "3-mcz8vy9p9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Результаты обучения 4 слоёв vs 6 слоёв"
      ],
      "metadata": {
        "id": "i_J4Js4H-Fwg"
      },
      "id": "i_J4Js4H-Fwg"
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/stylegan_nada_project\"\n",
        "\n",
        "for d in sorted(os.listdir(BASE_DIR)):\n",
        "    print(d)"
      ],
      "metadata": {
        "id": "O9RLuNYK-Kd3"
      },
      "id": "O9RLuNYK-Kd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_K4 = \"/content/drive/MyDrive/stylegan_nada_project/sketch_late_only_v0/samples\"\n",
        "DIR_K6 = \"/content/drive/MyDrive/stylegan_nada_project/sketch_late_only_v0_exp2_more_layers_20260202_010304/samples\"\n",
        "\n",
        "samples_k4 = sorted(glob.glob(os.path.join(DIR_K4, \"sample_*.png\")))\n",
        "samples_k6 = sorted(glob.glob(os.path.join(DIR_K6, \"sample_*.png\")))\n",
        "\n",
        "img_k4 = Image.open(samples_k4[-1])\n",
        "img_k6 = Image.open(samples_k6[-1])\n",
        "\n",
        "print(\"k = 4\")\n",
        "display(img_k4)\n",
        "print(\"k = 6\")\n",
        "display(img_k6)\n"
      ],
      "metadata": {
        "id": "bKOnMv9UDKVl"
      },
      "id": "bKOnMv9UDKVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод\n",
        "\n",
        "Как и ожидалось, после расширения числа обучаемых блоков с 4 до 6 визуальный эффект скетч-стилизации стал более выраженным.\n",
        "Полученный результат можно считать приемлемым для поставленной задачи доменной адаптации, что подтверждает гипотезу о важности вклада не только поздних, но и части средних блоков генератора в перенос визуального стиля."
      ],
      "metadata": {
        "id": "o95DOsTVFNqa"
      },
      "id": "o95DOsTVFNqa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для чистоты эксперимента проведём генерацию новых изображений, не использовавшихся в процессе обучения.\n",
        "Для одних и тех же случайных латентных векторов сравним изображения, полученные из замороженной (G_frozen) и обучаемой (G_train) копий генератора."
      ],
      "metadata": {
        "id": "KOHONkbUGWZU"
      },
      "id": "KOHONkbUGWZU"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "new_imgs_frozen = []\n",
        "new_imgs_train = []\n",
        "\n",
        "for _ in range(4):\n",
        "    z = torch.randn([1, G_train.z_dim], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_f = G_frozen(\n",
        "            z, None,\n",
        "            truncation_psi=config[\"truncation\"],\n",
        "            noise_mode=\"const\"\n",
        "        )\n",
        "        img_t = G_train(\n",
        "            z, None,\n",
        "            truncation_psi=config[\"truncation\"],\n",
        "            noise_mode=\"const\"\n",
        "        )\n",
        "\n",
        "    new_imgs_frozen.append(to_uint8_hwc(img_f))\n",
        "    new_imgs_train.append(to_uint8_hwc(img_t))\n",
        "\n",
        "print(\"G_frozen\")\n",
        "display(Image.fromarray(make_grid(new_imgs_frozen, cols=4)))\n",
        "\n",
        "print(\"G_train (k = 6)\")\n",
        "display(Image.fromarray(make_grid(new_imgs_train, cols=4)))\n"
      ],
      "metadata": {
        "id": "AabqvGzjFoX8"
      },
      "id": "AabqvGzjFoX8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}